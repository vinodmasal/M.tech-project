# -*- coding: utf-8 -*-
"""LSTM Model Hyperparameter Tunning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/187AuGl-LTh0q_fc5g443nN3c1O8XGiCC

learning rate and momentum
"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import SimpleRNN
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU 
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import SGD

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# # scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7  # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(learn_rate=0.01):
	# create model
  # design network
  # First LSTM hidden layer
  model = Sequential()
  model.add(LSTM(units=40, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.2))

  model.add(Dense(1))
  optimizer = Adam(lr=learn_rate,)
  model.compile(loss='mae',metrics=['mse'], optimizer=optimizer)
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# X= scalerY.inverse_transform(train_X)
# Y= scalerY.inverse_transform(train_y)

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters

learn_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]

param_grid = dict(learn_rate=learn_rate)

grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

# X= scalerY.inverse_transform(train_X)
# Y= scalerY.inverse_transform(train_y)

grid_result = grid.fit(X, Y)
# # summarize results
# print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
# means = grid_result.cv_results_['mean_test_score']
# stds = grid_result.cv_results_['std_test_score']
# params = grid_result.cv_results_['params']
# for mean, stdev, param in zip(means, stds, params):
#     print("%f (%f) with: %r" % (mean, stdev, param))

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Learning Rate and Momentum.csv")
# files.download('LSTM_Learning Rate and Momentum.csv')

"""Neuron Activation Function"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM,SimpleRNN
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense
from keras.constraints import maxnorm

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7 # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(neurons=1):
	# create model
  # design LSTM network
  model = Sequential()
  model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.2))
  model.add(Dense(1))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer="Adam")
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
neurons = [10, 20, 30, 40, 50, 60 ]
param_grid = dict(neurons=neurons)

grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Network Weight Initialization.csv")
files.download('LSTM_Network Weight Initialization.csv')

"""Optimizer Tunning """

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7 #365 # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor


# Function to create model, required for KerasClassifier
def create_model(optimizer='adam'):
	# create model
  # design LSTM network
  model = Sequential()
  model.add(LSTM(units=40, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.2))

  model.add(Dense(1, activation='sigmoid'))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer=optimizer)
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
param_grid = dict(optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Network Weight Initialization.csv")
files.download('LSTM_Network Weight Initialization.csv')

"""Epoch and Batch size"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import SimpleRNN
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7  # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
# Function to create model, required for KerasClassifier
def create_model():
	# create model
  # design LSTM network
  model = Sequential()
  model.add(LSTM(units=40, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.2))
 
  model.add(Dense(1))
  model.compile(loss='mae',metrics=['mse',nse], optimizer='Adam')
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
X = train_X
Y = train_y
# create model
model = KerasRegressor(build_fn=create_model, verbose=0)
# define the grid search parameters
batch_size = [32, 64, 128, 256, 512]
epochs = [10, 50, 100, 150, 200]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)
grid_result = grid.fit(X, Y)
# summarize results
# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Batch size and epoch.csv")
files.download('LSTM_Batch size and epoch.csv')

"""Tune Network Weight Initialization

"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import SimpleRNN
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7  # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(init_mode='uniform'):
	# create model
  # design LSTM network
  model = Sequential()
  model.add(LSTM(units=40, kernel_initializer=init_mode, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.0))

  model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer="Adam")
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
param_grid = dict(init_mode=init_mode)
grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Network Weight Initialization.csv")
# files.download('LSTM_Network Weight Initialization.csv')

"""Tune the Neuron Activation Function"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7 # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
	# create model
  # design LSTM network
  model = Sequential()
  model.add(LSTM(units=40, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.0))

  model.add(Dense(1))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer="Adam")
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
# define the grid search parameters
activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
param_grid = dict(activation=activation)

grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Neuron Activation Function.csv")
files.download('LSTM_Neuron Activation Function.csv')

"""Tune Dropout Regularization"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense
from keras.constraints import maxnorm

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7  # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(dropout_rate=0.0, weight_constraint=0):
	# create model
  # design network
  model = Sequential()
  model.add(LSTM(units=40, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(dropout_rate))

  model.add(Dense(1))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer="Adam")
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
weight_constraint = [1, 2, 3, 4, 5]
dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)

grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM Regularization.csv")
files.download('LSTM Regularization.csv')

"""Tune the Number of Neurons"""

from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
import joblib
from google.colab import files
import numpy as np
import keras.backend as K
from keras.layers import Dense
from keras.constraints import maxnorm

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dataset = read_csv('maithondam.csv', header=0, index_col=0, parse_dates=True)
dataset.index = pd.date_range('01-01-1980','31-12-2016')
dataset['time'] = dataset.index.dayofyear
dataset

values = dataset.values
reframed = series_to_supervised(values, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[12,19]], axis=1, inplace=True)
# print(reframed.head())
reframed = reframed.values

X = reframed[:, (0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17)]
y = reframed[:, 11]

# scalerX = StandardScaler()
scalerX = MinMaxScaler(feature_range=(0, 1))
X = scalerX.fit_transform(X)

scalerY = MinMaxScaler(feature_range=(0, 1))
y = scalerY.fit_transform(y.reshape(-1, 1))

XF = []
yF = []

time_step = 7  # This value can be tested 
for i in range(0,len(X)-time_step): 
  try:
    XF.append(X[i:i+time_step])
    yF.append(y[i+time_step])
  except:
    pass

XF = np.array(XF)
yF = np.array(yF).reshape(-1, 1)
# split into train test sets
n_train_hours = 10811
train_X, test_X = XF[:n_train_hours, :], XF[n_train_hours:, :]
train_y, test_y = yF[:n_train_hours, :], yF[n_train_hours:, :]
print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)

def nse(y_true, y_pred):
    return 1-(K.sum((y_pred-y_true)**2)/(K.sum((y_true-K.mean(y_true))**2)))

import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
# from keras.wrappers.scikit_learn import KerasClassifier
from keras.wrappers.scikit_learn import KerasRegressor
#from scikeras.wrappers import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(neurons=1):
	# create model
  # design network
  model = Sequential()
  model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))
  model.add(Dropout(0.0))
  # # Second LSTM layer # 
  # model.add(LSTM(units=20))
  # model.add(Dropout(0.4))
  model.add(Dense(1))

  # Compile model
  model.compile(loss='mae',metrics=['mse'], optimizer="Adam")
  return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
# split into input (X) and output (Y) variables
X = train_X
Y = train_y

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, verbose=0)
# define the grid search parameters
neurons = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]
param_grid = dict(neurons=neurons)

grid = GridSearchCV(estimator=model, param_grid=param_grid, \
                    scoring =['neg_root_mean_squared_error','r2','neg_mean_absolute_error'], refit='neg_root_mean_squared_error', \
                    n_jobs=-1, cv=3)

grid_result = grid.fit(X, Y)

# save file in csv
results=pd.DataFrame()
results['mean_test_neg_mean_absolute_error'] = grid_result.cv_results_['mean_test_neg_mean_absolute_error']
results['mean_test_neg_root_mean_squared_error'] = grid_result.cv_results_['mean_test_neg_root_mean_squared_error']
results['mean_test_r2'] = grid_result.cv_results_['mean_test_r2']

results['params'] =  grid_result.cv_results_['params']
results.to_csv("LSTM_Number of Neurons in the Hidden Layer.csv")
files.download('LSTM_Number of Neurons in the Hidden Layer.csv')